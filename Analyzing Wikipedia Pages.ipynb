{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Analyzing Wikipedia Pages\n",
    "\n",
    "In this project, I will be working with data scraped from [Wikipedia](https://www.wikipedia.org/), a crowdsourced online encyclopedia. \n",
    "\n",
    "I will be analyzing 54 megabytes worth of articles to determine patterns in the Wikipedia writing and content presentation style. The data has been scraped by hitting random pages in Wikipedia, then downloading the contens using the `requests` package. The scraped data is in HTML format with embedded JavaScript code. \n",
    "\n",
    "The main goals of this project will be:\n",
    "\n",
    "* Extract only the text from the Wikipedia pages, and remove all HTML and Javascript markup.\n",
    "* Remove common page headers and footers from the Wikipedia pages.\n",
    "* Figure out what tags are the most common in Wikipedia pages.\n",
    "* Figure out patterns in the text.\n",
    "\n",
    "## Understanding the data\n",
    "\n",
    "I will begin by listing all of the files in the `wiki` folder, counting the number of files, and inspecting a single file to see if there are any patterns in the raw HTML. I will print only the first 100 characters of the random file I have chosen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in 'wiki' folder: 999 \n",
      "\n",
      "['wiki/Furubira_District,_Hokkaido.html', 'wiki/Valentin_Yanin.html', 'wiki/Kings_XI_Punjab_in_2014.html', 'wiki/William_Harvey_Lillard.html', 'wiki/Radial_Road_3.html'] \n",
      "\n",
      "<!DOCTYPE html>\n",
      "<html class=\"client-nojs\" lang=\"en\" dir=\"ltr\">\n",
      "<head>\n",
      "<meta charset=\"UTF-8\"/>\n",
      "<title\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"Number of files in 'wiki' folder:\", len(os.listdir(\"wiki\")), \"\\n\")\n",
    "\n",
    "filenames = []\n",
    "for file in os.listdir(\"wiki\"):\n",
    "    filenames.append(\"wiki/{}\".format(file))\n",
    "\n",
    "print(filenames[:5], \"\\n\")\n",
    "with open ('wiki/Furubira_District,_Hokkaido.html') as f:\n",
    "    example = f.read()\n",
    "    print(example[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As far as patterns, I can see that certain strings of symbols and letters are repeated frequently, such as `</th>`, `</tr>`, `<tr>`, `<td>`, `</ul>`, and `</div>`. \n",
    "\n",
    "## Reading in the data\n",
    "\n",
    "Now that I understand the file structure and the structure of a single file, I can read in all of the files. Since this task is I/O bound, I can use threads to help read in the data more quickly. I will also experiment with different thread counts to determine which number works best. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Furubira_District,_Hokkaido', 'Valentin_Yanin', 'Kings_XI_Punjab_in_2014', 'William_Harvey_Lillard', 'Radial_Road_3', 'George_Weldrick', 'Zgornji_Otok', 'Blue_Heelers_(season_8)', 'Taggen_Nunatak', '1951_National_League_tie-breaker_series']\n",
      "0.2934556007385254\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "import time\n",
    "\n",
    "pool = concurrent.futures.ThreadPoolExecutor(max_workers=2)\n",
    "\n",
    "def read_file(filename):\n",
    "    with open(filename) as f:\n",
    "        data = f.read()\n",
    "    return data\n",
    "\n",
    "start = time.time()\n",
    "filenames = []\n",
    "for file in os.listdir(\"wiki\"):\n",
    "    filenames.append(\"wiki/{}\".format(file))\n",
    "content = pool.map(read_file, filenames)\n",
    "content = list(content)\n",
    "\n",
    "articles = []\n",
    "for i in range(len(content)):\n",
    "    articles.append(filenames[i].replace(\"wiki/\", \"\").replace(\".html\", \"\"))\n",
    "    \n",
    "print(articles[:10])\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have found that two threads (`max_workers=2`) sufficiently handle all of this data. \n",
    "\n",
    "## Removing extraneous markup\n",
    "\n",
    "Now that I have read in the data files, I can remove the extraneous markup that's outside of the `div#content` tag that most of the data seems to be inside. I will use the [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) package for this, which will allow me to extract all of the content inside a specific tag. \n",
    "\n",
    "With this package, I will parse each wiki article then extract the div with id content and everything inside it. \n",
    "\n",
    "This operation is more CPU intensive than before, so I will use a process pool to improve speed as opposed to a thread pool. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53.37423658370972\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def parse_html(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    parsed_soup = str(soup.find_all(\"div\", id=\"content\")[0])\n",
    "    return parsed_soup\n",
    "\n",
    "start = time.time()\n",
    "pool = concurrent.futures.ProcessPoolExecutor(max_workers=3)\n",
    "parsed = pool.map(parse_html, content)\n",
    "parsed = list(parsed)\n",
    "\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like this is a fairly slow process no matter how many workers are used. The above function takes about 55 seconds to execute. Below I will print just the first 1000 characters of the first item in the `parsed` list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"mw-body\" id=\"content\" role=\"main\">\n",
      "<a id=\"top\"></a>\n",
      "<div id=\"siteNotice\"><!-- CentralNotice --></div>\n",
      "<div class=\"mw-indicators\">\n",
      "</div>\n",
      "<h1 class=\"firstHeading\" id=\"firstHeading\" lang=\"en\">Furubira District, Hokkaido</h1>\n",
      "<div class=\"mw-body-content\" id=\"bodyContent\">\n",
      "<div id=\"siteSub\">From Wikipedia, the free encyclopedia</div>\n",
      "<div id=\"contentSub\"></div>\n",
      "<div class=\"mw-jump\" id=\"jump-to-nav\">\n",
      "\t\t\t\t\tJump to:\t\t\t\t\t<a href=\"#mw-head\">navigation</a>, \t\t\t\t\t<a href=\"#p-search\">search</a>\n",
      "</div>\n",
      "<div class=\"mw-content-ltr\" dir=\"ltr\" id=\"mw-content-text\" lang=\"en\"><table class=\"plainlinks metadata ambox ambox-content ambox-Unreferenced\" role=\"presentation\">\n",
      "<tr>\n",
      "<td class=\"mbox-image\">\n",
      "<div style=\"width:52px\"><a class=\"image\" href=\"/wiki/File:Question_book-new.svg\"><img alt=\"\" data-file-height=\"399\" data-file-width=\"512\" height=\"39\" src=\"//upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-new.svg/50px-Question_book-new.svg.png\" srcset=\"//upload.wikimedia.org/wikipedia/en/\n"
     ]
    }
   ],
   "source": [
    "print(parsed[0][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding common tags\n",
    "\n",
    "I have now extracted the main part of each page, so I can count up how many times each tag occurs. This will give information about how Wikipedia pages are typically structured. For example, many `a` tags would tell me that articles tend to be very connected to other articles or pages. Many `div` tags would tell me that Wikipedia pages tend to have a nested structure with many page elements. \n",
    "\n",
    "This process will be CPU bound, so I will use processes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.30339241027832\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'a': 214557,\n",
       " 'abbr': 3665,\n",
       " 'annotation': 2,\n",
       " 'area': 39,\n",
       " 'audio': 2,\n",
       " 'b': 14455,\n",
       " 'bdi': 4,\n",
       " 'big': 75,\n",
       " 'blockquote': 58,\n",
       " 'body': 999,\n",
       " 'br': 4986,\n",
       " 'caption': 200,\n",
       " 'center': 64,\n",
       " 'cite': 3563,\n",
       " 'code': 108,\n",
       " 'dd': 1376,\n",
       " 'del': 2,\n",
       " 'div': 58927,\n",
       " 'dl': 457,\n",
       " 'dt': 334,\n",
       " 'font': 40,\n",
       " 'form': 999,\n",
       " 'h1': 999,\n",
       " 'h2': 5044,\n",
       " 'h3': 11954,\n",
       " 'h4': 117,\n",
       " 'h5': 4,\n",
       " 'h6': 1,\n",
       " 'head': 999,\n",
       " 'hr': 51,\n",
       " 'html': 999,\n",
       " 'i': 18246,\n",
       " 'img': 8699,\n",
       " 'input': 3996,\n",
       " 'label': 999,\n",
       " 'li': 133277,\n",
       " 'link': 12985,\n",
       " 'map': 2,\n",
       " 'math': 2,\n",
       " 'meta': 4499,\n",
       " 'mo': 2,\n",
       " 'mrow': 2,\n",
       " 'mstyle': 2,\n",
       " 'noscript': 999,\n",
       " 'ol': 858,\n",
       " 'p': 7998,\n",
       " 'pre': 1,\n",
       " 'q': 76,\n",
       " 'rb': 16,\n",
       " 'rp': 32,\n",
       " 'rt': 16,\n",
       " 'ruby': 16,\n",
       " 's': 10,\n",
       " 'samp': 2,\n",
       " 'script': 4995,\n",
       " 'semantics': 2,\n",
       " 'small': 3272,\n",
       " 'source': 2,\n",
       " 'span': 75342,\n",
       " 'strong': 599,\n",
       " 'sub': 151,\n",
       " 'sup': 11157,\n",
       " 'table': 4010,\n",
       " 'td': 57673,\n",
       " 'th': 14472,\n",
       " 'title': 999,\n",
       " 'tr': 27300,\n",
       " 'u': 51,\n",
       " 'ul': 24147,\n",
       " 'wbr': 85}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_tag_counts(document):\n",
    "    soup = BeautifulSoup(document, 'html.parser')\n",
    "    tags = {}\n",
    "    for tag in soup.find_all():\n",
    "        if tag.name not in tags:\n",
    "            tags[tag.name] = 0\n",
    "        tags[tag.name] += 1\n",
    "    return tags\n",
    "\n",
    "start = time.time()\n",
    "pool = concurrent.futures.ProcessPoolExecutor(max_workers=2)\n",
    "tags = pool.map(find_tag_counts, content)\n",
    "tags = list(tags)\n",
    "\n",
    "total_tag_counts = {}\n",
    "for tag in tags:\n",
    "    for k,v in tag.items():\n",
    "        if k not in total_tag_counts:\n",
    "            total_tag_counts[k] = 0\n",
    "        total_tag_counts[k] += v\n",
    "        \n",
    "print(time.time() - start)\n",
    "total_tag_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon inspecting the tag counts, I see that some of the most common tags are `a` and `li`. This indicates that there are many hyperlinks throughout the articles, and that there are many list items. This is fairly close to what I would expect, since it is very common to see links and lists on Wikipedia pages. \n",
    "\n",
    "## Finding common words\n",
    "\n",
    "After finding common tags, I am now able to find common words in the article body. The criteria that I will use to determine if characters form a word are the following: \n",
    "\n",
    "* There are more than 5 letters. The reason for this is to exclude words like \"a\" and \"the\", since they do not necessarily give insight into trends in text. \n",
    "* They use the characters A-Z, a-z, 0-9, or _.\n",
    "\n",
    "I will replace all other characters with a space, and will also make all words lowercase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.61172318458557\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('title', 153011),\n",
       " ('class', 146280),\n",
       " ('style', 69490),\n",
       " ('width', 35321),\n",
       " ('wikipedia', 30397),\n",
       " ('height', 26472),\n",
       " ('border', 25620),\n",
       " ('align', 24089),\n",
       " ('category', 19921),\n",
       " ('template', 19570),\n",
       " ('padding', 18893),\n",
       " ('wikimedia', 16990),\n",
       " ('thumb', 16990),\n",
       " ('upload', 16759),\n",
       " ('index', 16053),\n",
       " ('navbox', 15531),\n",
       " ('reference', 15261),\n",
       " ('cite_ref', 14867),\n",
       " ('action', 13849),\n",
       " ('commons', 13463)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def find_word_counts(document):\n",
    "    soup = BeautifulSoup(document, 'html.parser')\n",
    "    parsed_soup = str(soup.find_all(\"div\", id=\"content\")[0])\n",
    "    parsed_soup = parsed_soup.lower()\n",
    "    data = re.sub('\\W+', \" \", parsed_soup)\n",
    "    data = data.split(\" \")\n",
    "    words = []\n",
    "    for word in data:\n",
    "        if len(word) >= 5:\n",
    "            words.append(word)\n",
    "    count = Counter(words)\n",
    "    return dict(count)\n",
    "\n",
    "start = time.time()\n",
    "pool = concurrent.futures.ProcessPoolExecutor(max_workers=2)\n",
    "words = pool.map(find_word_counts, content)\n",
    "words = list(words)\n",
    "\n",
    "total_word_counts = {}\n",
    "for word in words:\n",
    "    for k,v in word.items():\n",
    "        if k not in total_word_counts:\n",
    "            total_word_counts[k] = 0\n",
    "        total_word_counts[k] += v\n",
    "        \n",
    "print(time.time() - start)\n",
    "top_20 = Counter(total_word_counts).most_common(20)\n",
    "top_20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance time above could decrease if I were to select only the top 100 words from each article, and have only those returned in the function `find_word_counts`. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
