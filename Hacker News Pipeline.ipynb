{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Hacker News Pipeline\n",
    "\n",
    "In this project, I will begin with raw data then filter, clean, aggregate, and summarize it in a sequence of tasks using a data pipeline. The dataset I will be using comes from a [Hacker News](https://news.ycombinator.com/) API that returns JSON data of the top stories in 2014. The JSON posts are in a file called `hn_stories_2014.json`. Each post has a set of keys, and I am interested in the following keys:\n",
    "\n",
    "* `created_at`: A timestamp of the story's creation time.\n",
    "* `created_at_i`: A unix epoch timestamp.\n",
    "* `url`: The URL of the story link.\n",
    "* `objectID`: The ID of the story.\n",
    "* `author`: The story's author (username on HN).\n",
    "* `points`: The number of upvotes the story had.\n",
    "* `title`: The headline of the post.\n",
    "* `num_comments`: The number of a comments a post has.\n",
    "\n",
    "With this dataset, I will run a sequence of basic natural language processing tasks using the `Pipeline` class. My goal will be to find the top 100 keywords of Hacker News posts in 2014. This will give insight into some of the most popular tech topics in that year.\n",
    "\n",
    "I will create pipieline tasks that do the following:\n",
    "\n",
    "1) Loads the JSON file data into Python by parses it into a Python `dict` object\n",
    "\n",
    "2) Filters popular stories by posts that have more than 50 points, more than one comment, and do not begin with \"Ask HN\" \n",
    "\n",
    "3) Writes the `dict` objects to a CSV file in order to have a consistent data cormat when running later summarizations\n",
    "\n",
    "4) Extracts the title column from each post and runs the next word frequency task\n",
    "\n",
    "5) Turns titles into all lowercase letters and removes punctuation, in order to recognize \"Title\" and \"title!\" as the same word\n",
    "\n",
    "6) Builds a word frequency dictionary with key value pairs that connect words to the number of times that word is used in a text, as well as excludes stop words such as \"the, \"or\", etc.\n",
    "\n",
    "7) Sorts the top words used in all the titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import io\n",
    "import string\n",
    "from datetime import datetime\n",
    "from pipeline import build_csv, Pipeline\n",
    "from stop_words import stop_words\n",
    "\n",
    "pipeline = Pipeline()\n",
    "\n",
    "@pipeline.task()\n",
    "def file_to_json():\n",
    "    with open('hn_stories_2014.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "        stories = data['stories']\n",
    "    return stories\n",
    "    \n",
    "@pipeline.task(depends_on=file_to_json)\n",
    "def filter_stories(stories):\n",
    "    def popular(story):\n",
    "        return story['points'] > 50 and story['num_comments'] > 1 and not story['title'].startswith(\"Ask HN\") \n",
    "    return (story for story in stories if popular(story))\n",
    "\n",
    "@pipeline.task(depends_on=filter_stories)\n",
    "def json_to_csv(stories):\n",
    "    lines = []\n",
    "    for story in stories:\n",
    "        lines.append((\n",
    "            story['objectID'], \n",
    "            datetime.strptime(story['created_at'], \"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "            story['url'],\n",
    "            story['points'],\n",
    "            story['title']\n",
    "        ))\n",
    "    csv_file = build_csv(\n",
    "        lines, \n",
    "        header=['objectID', 'created_at', 'url', 'points', 'title'],\n",
    "        file=io.StringIO()\n",
    "    )\n",
    "    return csv_file.readlines()\n",
    "\n",
    "@pipeline.task(depends_on=json_to_csv)\n",
    "def extract_titles(csv_file):\n",
    "    reader = csv.reader(csv_file)\n",
    "    header = next(reader)\n",
    "    idx = header.index('title')\n",
    "    return ((line[idx]) for line in reader)\n",
    "\n",
    "@pipeline.task(depends_on=extract_titles)\n",
    "def clean_titles(titles):\n",
    "    for title in titles:\n",
    "        title = title.lower()\n",
    "        title = ''.join(x for x in title if x not in string.punctuation)\n",
    "        yield title\n",
    "\n",
    "@pipeline.task(depends_on=clean_titles)\n",
    "def build_keyword_dictionary(titles):\n",
    "    word_frequency = {}\n",
    "    for title in titles:\n",
    "        for word in title.split(' '):\n",
    "            if word and word not in stop_words:\n",
    "                if word not in word_frequency:\n",
    "                    word_frequency[word] = 1\n",
    "                word_frequency[word] += 1\n",
    "    return word_frequency\n",
    "\n",
    "@pipeline.task(depends_on=build_keyword_dictionary)\n",
    "def get_top_100(word_freq):\n",
    "    all = [\n",
    "        (word, word_freq[word]) \n",
    "        for word in sorted(word_freq, key=word_freq.get, reverse=True)]\n",
    "    return all[:100]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the entire pipeline is built, it is time to run it and determine the top 100 words that were discussed in the Hacker News posts of 2014. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('new', 186), ('google', 168), ('bitcoin', 102), ('open', 93), ('programming', 91), ('web', 89), ('data', 86), ('video', 80), ('python', 76), ('code', 73), ('released', 72), ('facebook', 72), ('using', 71), ('2013', 66), ('javascript', 66), ('source', 65), ('free', 65), ('game', 64), ('internet', 63), ('microsoft', 60), ('c', 60), ('linux', 59), ('app', 58), ('pdf', 56), ('language', 55), ('work', 55), ('2014', 53), ('software', 53), ('startup', 52), ('use', 51), ('make', 51), ('apple', 51), ('yc', 49), ('time', 49), ('security', 49), ('github', 46), ('nsa', 46), ('windows', 45), ('world', 42), ('like', 42), ('way', 42), ('heartbleed', 41), ('project', 41), ('computer', 41), ('1', 41), ('git', 38), ('dont', 38), ('users', 38), ('design', 38), ('ios', 38), ('life', 37), ('os', 37), ('twitter', 37), ('developer', 37), ('ceo', 37), ('vs', 37), ('day', 36), ('big', 36), ('android', 35), ('online', 35), ('years', 34), ('simple', 34), ('court', 34), ('api', 33), ('guide', 33), ('mt', 33), ('browser', 33), ('learning', 33), ('apps', 33), ('says', 33), ('problem', 32), ('server', 32), ('mozilla', 32), ('site', 32), ('fast', 32), ('gox', 32), ('firefox', 32), ('engine', 32), ('amazon', 31), ('year', 31), ('introducing', 31), ('people', 30), ('text', 30), ('support', 30), ('stop', 30), ('built', 30), ('million', 30), ('better', 30), ('tech', 29), ('3', 29), ('does', 29), ('development', 29), ('website', 28), ('2048', 28), ('developers', 28), ('just', 28), ('did', 28), ('inside', 28), ('best', 28), ('money', 28)]\n"
     ]
    }
   ],
   "source": [
    "output = pipeline.run()\n",
    "print(output[get_top_100])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
